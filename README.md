# NLP Framework Lab
## Novembre 2023 

--FRANCAIS--

#### Résumé :

Ce projet a été mené dans le cadre du cours ALTeGrad - Apprentissage Avancé pour les Données Textuelles et Graphiques(cours commun entre le Master MVA et le Master 2 Data Science X). La conception de l'énoncé et de la trame des tâches ont été pensées par Dr. G. Shang, H. Abdine.
**Mots Clefs** *Fairseq, HuggingFace Transformers, Language Modeling, Sentiment Analysis*

#### Objectif :

Le principal objectif de ce projet tourne autour de l'utilisation de Fairseq et des transformers HuggingFace. L'objectif global est de se familiariser avec ces librairies et modèles, englobant les étapes de prétraitement des données et l'implémentation de modèle. Les tâches spécifiques comprennent :

1. Le réglage fin du modèle de langage pré-entraîné RoBERTa_fr sur l'ensemble de données d'analyse de sentiment CLS_Books.
2. Évaluation de l'impact du réglage fin par rapport à un modèle pré-entraîné.
3. Le réglage fin d'une variante de BLOOM sur un ensemble de données de questions/réponses.

#### Exécution du code :

L'exécution du code nécessite l'installation d'un certains nombre de librairies idoines et compatibles. En particulier les bonnes version de bitsandbytes sur GPU. Le script s'exécute intégralement via un clique bouton

---------------------------------------------

--ENGLISH--

#### Abstract:

This project was conducted as part of the ALTeGrad course - Advanced Learning for Text and Graph Data, a common teaching beetween Master MVA and Master 2 Data Science X . The formulation of the project statement and task framework was orchestrated by Dr. G. Shang and H. Abdine.
**Key words** *Fairseq, HuggingFace Transformers, Language Modeling, Sentiment Analysis*

#### Objective:

The primary focus of this project revolves around the utilization of Fairseq and HuggingFace transformers. The overarching goal is to gain familiarization with these models, encompassing the steps of data preprocessing and the implementation of a Language Model (LM). The specific tasks include:

1. Fine-tuning the pre-trained RoBERTa_fr language model on the sentiment analysis dataset CLS_Books.
2. Assessing the impact of fine-tuning in comparison to a pre-trained model.
3. Fine-tuning a variant of BLOOM on a question/answer dataset.

#### How to run the code

The execution of the code requires the installation of a certain number of suitable and compatible libraries, particularly the correct versions of bitsandbytes for GPU. The script runs entirely with a single button click.
