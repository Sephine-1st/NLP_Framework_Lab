# NLP Framework Lab
## Novembre 2023 

--FRANCAIS--

#### Résumé :

Ce projet a été mené dans le cadre du cours ALTeGrad - Apprentissage Avancé pour les Données Textuelles et Graphiques(cours commun entre le Master MVA et le Master 2 Data Science X). La conception de l'énoncé et de la trame des tâches ont été pensées par Dr. G. Shang, H. Abdine.
**Key words** *Fairseq, HuggingFace Transformers, Language Modeling, Sentiment Analysis*

#### Objectif :

Le principal objectif de ce projet tourne autour de l'utilisation de Fairseq et des transformers HuggingFace. L'objectif global est de se familiariser avec ces librairies et modèles, englobant les étapes de prétraitement des données et l'implémentation de modèle. Les tâches spécifiques comprennent :

1. Le réglage fin du modèle de langage pré-entraîné RoBERTa_fr sur l'ensemble de données d'analyse de sentiment CLS_Books.
2. Évaluation de l'impact du réglage fin par rapport à un modèle pré-entraîné.
3. Le réglage fin d'une variante de BLOOM sur un ensemble de données de questions/réponses.

#### Conclusion :

Le projet offre une enquête approfondie sur Fairseq et les transformers HuggingFace pour la modélisation du langage et l'analyse de sentiments. Les résultats fournissent des perspectives précieuses sur l'efficacité du réglage fin des modèles pré-entraînés pour des tâches spécifiques, éclairant les subtilités de l'adaptation du modèle et de l'évaluation des performances.

---------------------------------------------

--ENGLISH--

#### Abstract:

This project was conducted as part of the ALTeGrad course - Advanced Learning for Text and Graph Data, a common teaching beetween Master MVA and Master 2 Data Science X . The formulation of the project statement and task framework was orchestrated by Dr. G. Shang and H. Abdine.
**Mots Clefs** *Fairseq, HuggingFace Transformers, Language Modeling, Sentiment Analysis*

#### Objective:

The primary focus of this project revolves around the utilization of Fairseq and HuggingFace transformers. The overarching goal is to gain familiarization with these models, encompassing the steps of data preprocessing and the implementation of a Language Model (LM). The specific tasks include:

1. Fine-tuning the pre-trained RoBERTa_fr language model on the sentiment analysis dataset CLS_Books.
2. Assessing the impact of fine-tuning in comparison to a pre-trained model.
3. Fine-tuning a variant of BLOOM on a question/answer dataset.

#### Conclusion
